• Created a GitHub repo https://github.com/zengm71/Jigsaw-Multilingual-Toxic-Comment-Classification

• HW6 in w251 used BERT https://github.com/zengm71/michael_zeng_w251_hw/blob/master/hw06/BERT_classifying_toxicity_michael_zeng_v100.ipynb, but it looks like the single language BERT and it did not use TPU. 

• The two getting started workbooks on Kaggle are not super helpful:

	○ https://www.kaggle.com/kivlichangoogle/jigsaw-multilingual-preprocessing

	○ https://www.kaggle.com/kivlichangoogle/jigsaw-multilingual-getting-started

• Instead, I think these ones are better:

	○ https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta, which demos the use of TPU, and this one https://www.kaggle.com/theoviel/bert-pytorch-huggingface-starter is about TPU only.

	○ Multilingual DistilBERT: DistilBERT is 2 times faster and 25% lighter than multilingual BERT base, all while retaining 92% of its performance. This model let you quickly experiments with different ideas, and when you are ready for the real thing, just change two lines of code to use bert-base-multilingual-cased.
		
	https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras

• If we do well on the competition, we can extend and optimize our model for internal workshop for semantic evaluation http://alt.qcri.org/semeval2020/index.php?id=tasks, which is the cutting edge of NLP tasks there. And the related task will be Offenseval 2020 (https://sites.google.com/site/offensevalsharedtask/home), which is doing exactly the same thing but on a different dataset. 

To read:
	• The BERT paper https://www.aclweb.org/anthology/N19-1423.pdf
	
	• The M-BERT paper https://arxiv.org/abs/1906.01502
	https://github.com/google-research/bert/blob/master/multilingual.md
	
