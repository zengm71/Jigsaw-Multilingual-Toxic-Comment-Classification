%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}

\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebsssssssssddddddsssssffsssddddddhe titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Transfer Learning for Offensive Language Detection}
\author{Michael Zeng\\
  School of Information\\
  University of California, Berkeley \\
  {\tt zengm71@berkeley.edu} \\\And
  Yekun  Wang\\
  School of Information\\
  University of California, Berkeley \\
  {\tt myekun.wang@berkeley.edu} \\}

 

\date{}

\begin{document}
\maketitle
\begin{abstract}
We present a methodology for building an offensive language classifier and evaluating its zero-shot as well as few-shots performance on an unseen but related task set. We adapt and fine-tune the pre-trained contextual embeddings and weights from various state-of-the-art models including BERT (both base and large), multi-lingual transformers such as XLM and XLM RoBERTa large, as well as encoder-decoder framework such as T5. We fine-tune these models on Jigsaw Multilingual Toxic Comment Classification dataset, and use the Offensive Language Identification (OLID) dataset for transfer learning evaluation. Our best model, XLM RoBERTa large, achieves a 93.27\% AUC score against the private Jigsaw test set, which is much improved compared to 80.37\% from our base model BERT-base and and just shy of 95.36\% from the winning solution. Our best model achieved 0.6942 F1 score with zero-shot learning against the OLID dataset, which is on-par with the best score of 0.70 in the 2019 workshop \citep{zampieri-etal-2019-predicting}; and we showed the model could exceed that score by training on as little as 1k observations from the OLID dataset.

\end{abstract}

\section{Introduction}
Combining the right to freedom of speech and the protection of online anonymity, the issue of offensive language or language toxicity has become increasingly more pervasive among online communities, penetrating many of the large social media platforms. The widespread use of toxic language is not only harmful for user experience and retention, it is also linked to online harassment and bullying incidents, which could deeply impale the victims mentally. Organizations currently employ various semi-automated content moderation solutions which combine automated offensive language tagging along with human validation to police and to protect their user base. 

Generally speaking, offensive language is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. In the past, researchers have studied various neural and non-neural machine learning approaches to identify language toxicity \citep{schmidt-wiegand-2017-survey}. More recently, researches have experimented with deep neural networks models for classifying language toxicity. 

Recently, transformer-based models \citep{DBLP:journals/corr/VaswaniSPUJGKP17} have been widely used in various areas of NLP research and applications. Depending on model specifications, training this kind of model could require estimating hundreds of millions to billions of parameters. Due to their massive size, many NLP practitioners prefer to build upon a pre-trained transformer model by stacking an extra layer on top, which adapts the existing model for task-specific purposes. This paper aims to evaluate the various transformer-based models, such as BERT-base and BERT-large \citep{DBLP:journals/corr/abs-1810-04805}, XLM \citep{DBLP:journals/corr/abs-1901-07291}, XLM RoBERTa \citep{conneau2019unsupervised}, T5 \citep{raffel2019exploring}; fine-tuning them to detect offensive language on the Jigsaw Multilingual Toxic Comment Classification dataset\footnote{https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/data}.

With the emergence of transformer-based models, comes the demand for increasingly larger datasets to train them on; however, labeled truth datasets are scarce and are costly to scale. Therefore, another goal of this study is to demonstrate the effectiveness of sequential transfer learning techniques in resolving the practical issue of labeled data scarcity. The general idea behind transfer learning is to take parameters or knowledge from one trained model and apply it to another \citep{ruder-etal-2019-transfer}. In this study, we experiment with both zero-shot and few-shot learning by applying the offensive language classifier trained on the Jigsaw dataset and evaluate the model's performance against the OLID dataset \citep{zampieri-etal-2019-predicting}. 

This paper’s contribution to this area are three-fold: 

\begin{itemize}
  \item We adapt and compare various pre-trained state-of-the-art deep neural network models to the task of offensive language identification.
  \item We demonstrate zero-shot and few-shot transfer learning techniques by applying the fine-tuned offensive language classifier to a new dataset and evaluate its general performance. 
  \item We show that most models see diminishing returns as more target dataset is exposed, and reasonable transfer learning experience could be achieved with limited data. 
\end{itemize}

The paper is organized as follows. In section 1, we discuss the existing work related to offensive language identification using various NLP techniques. In section 2, we provide details of the transformer models employed in our experiments. In section 3, we discuss the show results of fine-tuning various transformer models on the Jigsaw dataset. In section 4, we discuss and show results for zero-shot and few-shot learning of applying the fine-tuned offensive language model to the OLID dataset. In section 5 and 6, we provide an error analysis and summarize the entire study. 


\subsection{Existing Work}

Earlier research, \citep{6406271} and \citep{huang}, looked into feature-based language models for identifying online bullying and hate speech. More recently, \citep{zampieri-etal-2019-predicting} compiled the OLID dataset and applied SVM, CNN, BiLSTM models to predicting the offensive language label. The best performing CNN model achieved 0.70 F1 score on the positive label of the OLID dateset. \citep{kohli} demonstrated a deep learning approach using word-level and character-level RNNs with custom embeddings on the Jigsaw data\footnote{https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data} from the 2018 Kaggle competition. Additionally, \citep{ruder-etal-2019-transfer} outlined the landscape of various transfer learning techniques pertaining to NLP tasks.  

\subsection{Data}
First part of this study focuses on the Jigsaw Multilingual Toxic Comment Classification dataset, published as a part of a Kaggle competition\footnote{https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/data}. The data consists of English comments from Wikipedia’s talk page edits as well as an expanded version of the Civil Comments dataset. 

Due to the limit on resources, we constructed our Jigsaw training set by taking the entire 2018 dataset, and combined that with all offensive tweets and down-sampled non-offensive ones from the 2019 dataset. The constructed dataset has close to 500K observations with around 133k being offensive, with a targeted positive rate of 30\%.

Interestingly, Jigsaw’s training dataset is English-only, but the test dataset contains other languages. The organizer intended it this way to validate zero-shot transfer learning of models trained on English and apply them to other languages. We take this idea of transfer learning one step further and focus the second part of the paper on validating zero-shot and few-shot transfer learning on the OLID Dataset.

The OLID Dataset is provided by the International Workshop on Semantic Evaluation as part of a 2019 competition \citep{zampieri-etal-2019-predicting}, which contains a collection of 14,200 annotated English tweets. 

\begin{table}
\centering
\begin{tabular}{c|cc}
\textbf{DATASET} & \textbf{Jigsaw}                                                        & \textbf{OLID}                                                              \\ \hline
\textbf{TRAIN}   & \begin{tabular}[c]{@{}c@{}c@{}}485,775\\ 133,610 positive \\English Only\end{tabular} & \begin{tabular}[c]{@{}c@{}c@{}}10,000\\ 1,231 positive \\English Only\end{tabular} \\ \hline
\textbf{DEV}     & \begin{tabular}[c]{@{}c@{}c@{}}8,000\\ 1,230 positive \\ Multi-lingual\end{tabular}     & \begin{tabular}[c]{@{}c@{}c@{}}4,200\\ 1,105 positive\\ English Only\end{tabular}  \\ \hline
\textbf{TEST}    & \begin{tabular}[c]{@{}c@{}c@{}}63,812\\ unknown positive\\ Multi-lingual\end{tabular}  & \begin{tabular}[c]{@{}c@{}c@{}}860\\ 240 positive\\ English Only\end{tabular}   \\ \hline
\end{tabular}
\caption{Dataset Summary}\label{tab:dataset}
\end{table}

To compare our model performance against other participants’ submissions, we used the same evaluation metrics mandated by the competition to train our models -- AUC for the Jigsaw dataset and F1 score for the OLID dataset. Table \ref{tab:dataset} gives the detailed breakdown of dataset used for experiments.

\subsection{Infrastructure}
We choose the Kaggle notebook as the main working environment, because Kaggle allows 30 hours of TPU V3 and V100 GPU time per person per week, an incredible resource for free access to the state-of-the-art hardware. In comparison, the TPU V2 offered by Google Colab Pro, which only has half the memory of V3, sometimes fail to fit any training example at all after loading a large model. Comparing TPU vs GPU, we find that TPU offers a significant boost to the training time that’s at least 3-5 times shorter than training on V100.

\section{Models}
In this study, we experimented with fine-tuning five transformer models to the offensive language classification task. In the fine-tuning phase, the model was initialized with the pre-trained parameters and then fine-tuned on the labelled Jigsaw dataset. All models implementations were taken from HuggingFace \citep{wolf2019huggingfaces}, an open-source library for many state-of-the-art transformer architectures under a unified API. We experimented with three suits of models: (i) BERT and BERT large, which are faithful implementations of the original BERT models; (ii) multi-language BERT such as XLM and XLM-RoBERTa; (iii) encoder-decoder framework such as T5. 

\subsection{BERT}
BERT \citep{DBLP:journals/corr/abs-1810-04805}, short for Bidirectional Encoder Representations from Transformers, is a bidirectional transformer pre-trained on English language using a combination of masked language modeling (MLM) and next sentence prediction (NSP). We leveraged the CLS tokens from the raw model and specify it to the downstream offensive language classification task. We picked the case-sensitive models based on the observation that upper-casing generally imply strong emotions in tweets, which should help with our objective of offensive language detection. We included both BERT-base as well as BERT-large to gauge the improvement in performance against the size of the model. Both model implementations, as published on HuggingFace, were pre-trained on the BookCorpus and English Wikipedia data.

\subsection{Multi-language BERT}
The work on cross-lingual language models (XLMs) \citep{DBLP:journals/corr/abs-1901-07291} extends the generative pretraining for English language understanding to multiple languages and shows the effectiveness of cross-lingual pretraining. Another variation we picked in our experiment is XLM-RoBERTa model \citep{conneau2019unsupervised}, based on Facebook’s RoBERTa model released in 2019, and trained on 2.5TB of filtered CommonCrawl data. 

\subsection{T5}
Asides from BERT or variations of BERT that build on the encoder part of the transformer architecture, we also experimented with T5, an encoder-decoder model pre-trained on a variety of tasks that had been converted into text-to-text format. The training of T5 differed from the BERT based model -- we fed the model with pairs of token and label directly, instead of extracting the CLS tokens and stacking a softmax layer on top for classification. As detailed in \citep{raffel2019exploring}, the T5 model was pre-trained on 750GB of natural English text, obtained by web crawling.


\section{Jigsaw Performance}
We experimented with fine-tuning various pre-trained transformer models on the Jigsaw Multilingual Toxic Comment Classification dataset for the language offensive language task. For the Jigsaw dataset, we trained each model on the training set for 3 epochs with 1e-5 learning rate. Since the training set is English only, we then trained each model for 3 additional epochs on the dev set which contains non-English language. Table \ref{tab:jigsaw_performance} reports the final AUC scores as calculated base on the public and private Kaggle test set.

One way to use the BERT style models is to add a sigmoid affine layer on top of the CLS tokens to fine tune these transformer models. Depending on constraints on computational resources, another way of applying these models could be to keep the transformer weights frozen or partially frozen, as demonstrated by \citep{lee2019elsa} to have limited reduction in performance. Gaining access to the TPU units allowed us to experiment with freezing or unfreezing the transformer weights and to evaluate various models more comprehensively both in terms of model performance as well as run time. %Yet, due to limited access to TPU, we only experimented with model weights full frozen or unfrozen, instead of freezing only a fraction of the layers. 
We report the models' performance on the Jigsaw public and private dataset in Table \ref{tab:jigsaw_performance}, using AUC as the metric to be consistent with the competition. 

% Please add the following required packages to your document preamble:
%\usepackage{multirow}
\begin{table*}[]
\centering
\begin{tabular}{l|lccc}
\multicolumn{1}{l|}{\textbf{MODEL}} &
  \multicolumn{1}{l}{\textbf{\begin{tabular}[l]{@{}l@{}}Freeze\\ Transformer\end{tabular}}} &
  \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Jigsaw Public \\ Score (AUC)\end{tabular}}} &
  \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Jigsaw Private \\ Score (AUC)\end{tabular}}} &
  \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Training \\ Time (s)\end{tabular}}} \\ \hline
\multirow{\textbf{BERT   Base}}       & No  & 0.8037 & 0.8026 & 1,452 \\
                                            & Yes & 0.8044 & 0.8011 & 1,413 \\ \hline
\multirow{\textbf{BERT   Large}}      & No  & 0.8184 & 0.8167 & 3,300 \\
                                            & Yes & 0.8089 & 0.8041 & 3,423 \\ \hline
\multirow{\textbf{XLM}}               & No  & 0.9148 & 0.9121 & 2,859 \\
                                            & Yes & 0.9096 & 0.9094 & 2,901 \\ \hline
\multirow{\textbf{XLM RoBERTa Large}} & No  & \textbf{0.9335} & \textbf{0.9327} & 5,205 \\
                                            & Yes & 0.9306 & 0.9315 & 5,115 \\ \hline
\multirow{\textbf{T5 Large}}          & No  & 0.8392 & 0.8373 & 8,844 \\
                                            & Yes & 0.8371 & 0.8352 & 8,925 \\ \hline 
\textbf{2020 Winning Solution}          & --  & 0.9556 & 0.9536 & -- \\\hline                                             
\end{tabular}
\caption{Performance on Jigsaw Dataset and Training Time for 3 Epochs}\label{tab:jigsaw_performance}
\end{table*}

We saw an intuitive improvement in performance from unfreezing the transformer weights. By unfreezing the transformer weights, we allowed many more degrees of freedom while training the model. Yet we saw the increase in model power generalizes very well on the test set. On the other hand, unfreezing the transformer weights consumed more memory on the TPU unit, and thus required lower batch sizes. Yet from our experiment, the training time stayed largely the same, which prompts us to leave the transformer weights unfrozen for the rest of the experiments. 

Comparing across models, the gain from BERT to BERT large is only marginal despite the significant size-up and increase in training time. The big leap in performance come from the switching from English to multi-language models such as XLM and XLM RoBERTa large. Given the fact that both the public and private test sets are multilingual, the English only embedding used in BERT Base, BERT Large as well as T5 large is limiting the model performance. The improvement from XLM to XLM RoBERTa Large could be attributed to the pretraining of XLM Roberta Large, where the model was exposed to and trained on significantly larger corpus. Our best score is just shy of the 2020 winning solution where predictions from several models are further ensembled \footnote{https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/leaderboard}.

\section{Transfer Learning}
% The focus of this paper is to demonstrate the application and extensibility of the Transformer architecture to the text classification problem. Real-world NLP applications are oftentimes limited by the amount of data and computational resources to properly train a deep neural net NLP model. Instead of starting from scratch, a more preferred strategy is to take a model pre-trained on a large body of text and fine-tune it for a specific NLP task. Effectively, it is a type of transfer learning based on the assumption of a commonality in basic language understanding between the pre-trained dataset and the target task dataset. 

In this section, we explore the differences in zero-shot transfer learning performances from multiple models on the OLID dataset. It could be a reasonable baseline for the model’s ability to generalize to unseen data, given that the model was previously fine-tuned on a very similar task. We then explore the performances of few-shot learning by incrementally increase the portion of the OLID data for the model to be trained on. Given that we had 10K in total from the OLID training set, the choice of exposure was set to follow the geometric sequence: 1K, 2K, 5K and 10K. Since these chosen sizes are still small, we found training loss to be quite jumpy across epoches. As a result, for this section of training, we lowered the learning rate to 2.00E-06 and trained up to 10 epochs with a tight early-stop that checks the loss on dev set every 3 epochs. The hypothesis is that since the model has been pretrained on similar tasks, one may only need to expose a fraction of the dataset to obtain a high level of performance.

\begin{table*}[h]
\centering
\begin{tabular}{l|lccccc}
\textbf{Model}                              & \textbf{Freeze} & \textbf{OLID 0-shot} & \textbf{OLID 1k} & \textbf{OLID 2k} & \textbf{OLID 5k} & \textbf{OLID 10k (Full)} \\ \hline
\multirow{\textbf{BERT Base}}  & No  & 0.6690 & 0.6847 & 0.6821 & 0.6944 & 0.7091 \\
                               & Yes & 0.6226 & 0.6283 & 0.6283 & 0.6455 & 0.6750  \\ \hline
\multirow{\textbf{BERT Large}} & No  & 0.6919 & \textbf{0.7018} & \textbf{0.7065} & \textbf{0.7253} & \textbf{0.7330} \\
                               & Yes & 0.6619 & 0.6595 & 0.6596 & 0.6619 & 0.6690 \\ \hline
\multirow{\textbf{XLM}}        & No  & 0.6502 & 0.6655 & 0.6786 & 0.6969 & 0.7055 \\
                               & Yes & 0.6283 & 0.6285 & 0.6490 & 0.6513 & 0.6537 \\ \hline
\multirow{\textbf{XLM RoBERTa Large}} & No              & 0.6631               & 0.6762           & 0.7029           & 0.7091           & 0.7104            \\
                               & Yes & \textbf{0.6942} & 0.6895 & 0.6980 & 0.7029 & 0.7042 \\ \hline
\multirow{\textbf{T5}}         & No  & 0.6675 & 0.6838 & 0.6994 & 0.7072 & 0.7091 \\
                               & Yes & 0.6664 & 0.6663 & 0.6667 & 0.6669 & 0.6683 \\ \hline
\end{tabular}
\caption{Performance of Transfer Learning on OLID Dataset}\label{tab:olid_performance}
\end{table*}

\begin{figure}[h]
\includegraphics[width=\columnwidth]{transfer_learning_performance_nominal.png}
\centering
\caption{Performance of Transfer Learning on OLID Dataset}\label{fig:olid_performance_fig}
\end{figure}

Table \ref{tab:olid_performance} summarises the F1 score on the OLID test set from various experiments, which are also visualized in Figure \ref{fig:olid_performance_fig}. Our observations on the choice of freezing the transformer weights from the previous section still hold: most models perform better when trained with weights from transformer layer unfrozen. When they are frozen, the performance curves stayed mostly flat as more destination dataset is exposed. This indicates a lack of flexibility to adapt and learn from the new dataset with only the last layer being trainable.  

Across the different models, we find that BERT-large consistently outperforms the others across the zero-shot and few-shot experiments. The multi-lingual models such as XLM and XLM RoBERTa Large no longer demonstrate a performance advantage over others since the OLID test set is English only.

As more observations are exposed to the model, the performances continue to improve with a diminshing rates, as shown in Figure \ref{fig:olid_performance_fig}. For most models trained with weights from the transformation layer unfrozen, exposing the entire training set helps to achieve about a 4\% gain in F1 score compare to doing zero-shot inference, yet most of the gain was achieved from the first 5k observations. This is similar to the findings in \citep{cer2018universal}, where the author demonstrated that the performances would plateau as more labeled examples are exposed to the model. However, we are still trying to procure more labeled data on offensive language detection so that we can expand our experiment beyond 10k observations to observe the curve over large ranges.  

\section{Conclusion}
We explored the both zero and few-shots learning performances across several popular transformer based models for the common task of offensive language detection. We benchmarked the performance of sevreal state-of-the-art models on the Jigsaw dataset, and explored the choice of freezing the original model weights. Our experiments show unfreezing the weights from transformer layer, if resources allow, greatly boosts the model's ability to generalize and perform on unseen dataset. In terms of few-shots learning, we observed diminishing returns from exposing the model to additional training examples, as cited in \citep{cer2018universal}. However, the performance of zero or few-shots learning are also largely limited by the language in which the model was originally pretrained on: XLM RoBERTa has superior performance on Jigsaw test set which is multi-lingual, while BERT performs the best on OLID test set which is English only. 

\bibliography{acl2020}
\bibliographystyle{acl_natbib}
\end{document}
